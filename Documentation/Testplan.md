# Test plan

## 1.	Introduction
### 1.1	Purpose
The purpose of the Iteration Test Plan is to gather all of the information necessary to plan and control the test effort for a given iteration. 
It describes the approach to testing the software.
This Test Plan for the Deminder Project supports the following objectives: 
 
Our most important test case is to make sure, that all information the user insert into the app will be saved and shown correctly in the app.  
The idea is to write a JUnit test that creates a deadline object with all information that could be in a deadline and to compare the specific attributes of it with our insertions.
### 1.2	Scope
The functions are tested using JUnit tests. The user interface is checked for its correctness via the feature files. In addition, user tests are carried out to check whether the app meets our requirements as well as those of the user. 
### 1.3	Intended Audience
Our test plan is for everyone who wanted to understand the tests we did to provide the correctness of our project or who want to test by themselves to make sure that everything works like we expected or wants to increase the amount of tests we did. 
### 1.4	Document Terminology and Acronyms
- **Deminder** The app that is created in this project 
- **n.a**	not applicable
### 1.5	 References
n.a

## 2.	Evaluation Mission and Test Motivation
### 2.1	Background
The app is designed to make scheduling easier for users so that no deadlines can be missed. With the corresponding widget you have the possibility to place the deadlines, including their subtasks, always in the field of view in the middle of the start screen. By the possibility of the import and export of dates, one can transfer also Deadlines from and into other calendars problem-free. 
### 2.2	Evaluation Mission
- If the name is saved correctly 
- If the deadline date is saved correctly 
- If the notes are saved correctly 
- If the subtasks are saved correctly 
- If app is intuitive for user 
- If the components are on their right position 
- If all components do what they are supposed to do 
### 2.3	Test Motivators
Our test motivation is to make sure that everything works correctly, the design is clearly and the user knows intuitive how to use the app. 
## 3.	Target Test Items
The listing below identifies those test items, software, hardware, and supporting product elements that have been identified as targets for testing. This list represents what items will be tested. 

- Android Smartphone 
## 4.	Outline of Planned Tests
### 4.1	Outline of Test Inclusions
- Logic test with JUnit
- UI test with feature files
- User tests with people
### 4.2	Outline of Other Candidates for Potential Inclusion
n.a
## 5.	Test Approach
See 1.0
### 5.1	Testing Techniques and Types
#### 5.1.1	Function and Database Integrity Testing
n/a
#### 5.1.2	Unit Testing
|| |
|---|---|
|Technique Objective  	| Testing of functionality with JUnit |
|Technique 		|  Implementation of unit tests for specific use cases |
|Oracles 		|  Test expected and unexpected user inputs. Check for correct states afterwards.  |
|Required Tools 	| Android studi and JUnit tests |
|Success Criteria	|    All tests pass     |
|Special Considerations	|     n.a          |
#### 5.1.3	Business Cycle Testing
n/a
#### 5.1.4	User Interface Testing
|| |
|---|---|
|Technique Objective  	| Testing of UI with feature files  |
|Technique 		|  Writing feature files for specific use cases   |
|Oracles 		|  Checking if UI components on the right position and are working correctly |
|Required Tools 	| Android Studio and feature files |
|Success Criteria	|    All tests pass     |
|Special Considerations	|     n.a          |
#### 5.1.5	Performance Profiling 
n/a
#### 5.1.6	Load Testing
n/a
#### 5.1.7	Stress Testing
|| |
|---|---|
|Technique Objective  	| Testing how intuitive the use of our app is with people   |
|Technique 		|  Asking random people to use our app and see how good they understand it   |
|Oracles 		|  Give no input on how to use the app, let the user test the interface and behavior of the app.   |
|Required Tools 	| People with Android Smartphone |
|Success Criteria	|    Users think the app is intuitive |
|Special Considerations	|     n.a          |
#### 5.1.8	Volume Testing
n/a
#### 5.1.9	Security and Access Control Testing
n/a
#### 5.1.10	Failover and Recovery Testing
n/a
#### 5.1.11	Configuration Testing
n/a
#### 5.1.12	Installation Testing
n/a
## 6.	Entry and Exit Criteria
### 6.1	Test Plan
#### 6.1.1	Test Plan Entry Criteria
n.a
#### 6.1.2	Test Plan Exit Criteria
n.a
#### 6.1.3 Suspension and Resumption Criteria 
## 7.	Deliverables
### 7.1	Test Evaluation Summaries
Is automatically generated by Travis and coveralls. 
### 7.2	Reporting on Test Coverage
Test coverage is reported by coveralls.io.  
### 7.3	Perceived Quality Reports
Travis CI and Coveralls.io create badges which are displayed on the main page of the git repository. 
### 7.4	Incident Logs and Change Requests
n.a
### 7.5	Smoke Test Suite and Supporting Test Scripts
n.a
### 7.6	Additional Work Products
n.a
#### 7.6.1	Detailed Test Results
n.a
#### 7.6.2	Additional Automated Functional Test Scripts
n.a
#### 7.6.3	Test Guidelines
n.a
#### 7.6.4	Traceability Matrices
n/a
## 8.	Testing Workflow
JUnit was automatically integrated via gradle. All unit- and gherkin-tests are processed automatically when 

commiting and pushing to the master branch on the git repository via Travic CI.
## 9.	Environmental Needs
### 9.1	Base System Hardware
The following table sets forth the system resources for the test effort presented in this Test Plan. 

|| |
|---|---|
|Resource  	| Name and Type   |
|Developer Test PCs IDE  		|  Android Studio with Android phone emulator  |
|Client Test Device: Smartphone with OS 		|  Android Jelly Bean 4.2 and higher   |
### 9.2	Base Software Elements in the Test Environment
The following base software elements are required in the test environment for this Test Plan. 

|| |
|---|---|
|Software Element Name  	| Type and Other Notes   |
|Android Studio |  Integrated Development Environment   |
|Windows 10 / Debian 		|  Operating System   |
|Green coffee & Espresso | Software to run step definitions|
|Android | OS for the app |
| Travis CI | Environment for building, testing and developing deminder

### 9.3	Productivity and Support Tools
The following tools will be employed to support the test process for this Test Plan. 

- Project Management Tool YouTrack
## 10.	Responsibilities, Staffing, and Training Needs
### 10.1	People and Roles
This table shows the staffing assumptions for the test effort.

Human Resources


| Role | Minimum Resources Recommended (number of full-time roles allocated) |	Specific Responsibilities or Comments |
|---|---|---|
| Test Manager | 1 | Provides management oversight. <br> Responsibilities include: <br> planning and logistics <br> agree mission <br> identify motivators<br> acquire appropriate resources<br> present management reporting<br> advocate the interests of test<br>evaluate effectiveness of test effort |
| Test Designer | 1 | Defines the technical approach to the implementation of the test effort. <br> Responsibilities include:<br> define test approach<br> define test automation architecture<br> verify test techniques<br> define testability elements<br> structure test implementation|
| Tester | 1 |	Implements and executes the tests.<br> Responsibilities include:<br> implement tests and test suites<br> execute test suites<br> log results<br> analyze and recover from test failures<br> document incidents|
| Test System Administrator | 1 | Ensures test environment and assets are managed and maintained.<br> Responsibilities include:<br> 	administer test management system<br> install and support access to, and recovery of, test environment configurations and test labs | 
| Implementer | 4| Implements and unit tests the test classes and test packages.<br> Responsibilities include:<br> creates the test components required to support testability requirements as defined by the designer |
### 10.2	Staffing and Training Needs
n/a
## 11.	Iteration Milestones
| Milestone | Planned Start Date | Actual Start Date | Planned Start Date | Actual End Date |
|---|---|---|---|---|
|Iteration Plan agreed | 4.10.18 | 4.10.18 | 4.10.18 | 4.10.18 |
|Iteration starts | 4.10.18 | 4.10.18 | 4.10.18 | 4.10.18 | 
|Requirements baselined | 12.10.18 | 12.10.18 | 18.10.18 | 18.10.18 | 
|Architecture baselined | 18.10.18 | 18.10.18 | 25.10.18 | 25.10.18 | 
|User interface | 25.10.18 | 25.10.18 | 8.11.18 | 8.11.18 | 
|First Build delivered to test | 13.12.18 | 13.12.18 | 13.12.18 | 13.12.18 | 
|First Build test cycle finishes | 13.12.18 | 13.12.18 | 13.12.18 | 13.12.18 | 
|Second Build delivered to test | 5.6.19 | - | 5.6.19 |- | 
|Second Build accepted into test  | 12.6.19 | - | 12.6.19 | - | 
|Second Build test cycle finishes | 12.6.19 | -| 12.6.19| - | 
|Iteration Assessment review | 19.6.19 |  | 19.6.19 |  | 
|Iteration ends | 19.6.19 |  | 19.6.19 |  | 

## 12. Risks, Dependencies, Assumptions, and Constraints 
| Risk | Mitigation Strategy | Contingency (Risk is realized) |
|---|---|---|
| User in user tests does not understand the specaility of the app and therefore uses the app not in the intentended way | Tester will give the project team feedback on possibilities to make the app more understandable | Talk to tester to understand ways to better deminder |

| Assumption to be proven  | Impact of Assumption being incorrect | Owners |
|---|---|---|
| User understands how the app works  | User interface has to be redesigned  | Deminder Team  |
| Data is stored correctly on the app-running device   | Storing of data has to be reviewed and reimplemented   | Deminder Team  |

| Constraint on  | Impact Constraint has on test effort  | Owners |
|---|---|---|
| Code coverage of 10 % due to less logic and high usage of android interfaces  | More or less unit tests to write  | Deminder Team |

## 13. Management Process and Procedures 
### 13.1 Measuring and Assessing the Extent of Testing 
n.a 
### 13.2 Assessing the Deliverables of this Test Plan 
n.a 
### 13.3 Problem Reporting, Escalation, and Issue Resolution 
- Travis CI will generate a log
- The error gets reported by E-Mail to all group members if a test starts failing or the build is unstable
### 13.4 Managing Test Cycles 
n.a 
### 13.5 Traceability Strategies 
n.a 
### 13.6 Approval and Signoff 
Thomas Malina, Tillmann NÃ¼nninghoff, Natalie Busam, Lea Wegner
## 14. Metrics
We use the tool CodeMR for metrics. JetBrains offers a plugin for AndroidStudio. Through this, we are able to create metrics with one click.

We have run our metrics and they show this result:

![Metrics](https://raw.githubusercontent.com/Kalkihe/Deminder/master/Documentation/Screenshots/bevoremetrics.PNG)

We refactored our code to better the complexity and coupling. We like to mention that our 'worst' level is "medium-high" as you can see in the screenshot. This shows that our code was pretty decent in the first place. You can see our improvement here:

![Metrics](https://raw.githubusercontent.com/Kalkihe/Deminder/master/Documentation/Screenshots/aftermetrics.PNG)

We achieved this improvements by putting some (mostly layout-) functions outside of the big classes into smaller classes. You may see that Lack of Cohesion now increased. The reason for this is that, while improving on our metrics, there was further work done regarding the functionality of our app. This caused the creation of some methods, which are called by android itself and not by us. This is the reason that CodeMR highlights that methods.

Now we like to explain the metrics we used:

Complexity: Implies how difficult to understand a class is and the number of interactions between its entities.

Coupling: Describes the relationship between two classes (A and B). It has a high amount for example if A calls on services of an object B or A has a method that references B (for example as a return type).

![Metrics](https://raw.githubusercontent.com/Kalkihe/Deminder/master/Documentation/Screenshots/metric_nochanges.png)

As you can see, the class with the id 4 "DatePickerFragment" has a low-medium complexity and coupling. We are not going to change this because this is a library we use and didn't implement by ourselves.
